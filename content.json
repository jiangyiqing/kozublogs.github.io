{"meta":{"title":"Kozublog","subtitle":"A site of data science-related blogs","description":"","author":"jiangyiqing","url":"https://jiangyiqing.github.io/kozublogs.github.io","root":"/kozublogs.github.io/"},"pages":[{"title":"About Me","date":"2024-01-29T13:03:13.770Z","updated":"2024-01-28T13:56:54.395Z","comments":true,"path":"about.html","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/about.html","excerpt":"","text":""}],"posts":[{"title":"C Project - Using K-Nearest Neighbour to Classify Hand-Written Digits","slug":"knn-mnist","date":"2024-01-28T14:04:02.895Z","updated":"2024-01-30T12:13:34.942Z","comments":true,"path":"2024/01/28/knn-mnist/","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/2024/01/28/knn-mnist/","excerpt":"","text":"C Project TB1Using K-Nearest Neighbour algorithm to classify hand-written digits. What are we going to do?Although AI can nowadays do fancy work, such as creating images and generating articles, it started from humble beginnings. I mean, it all stems from certain types of neural networks and models. Back in the 90s, when algorithms, data, and, most importantly, hardware were less developed than today, people seldom used neural networks in applications because they cost too much. Instead, they use statistical models. In my opinion, statistical algorithms could also be viewed as a type of artificial intelligence. After all, they handle unseen data. So, what did people in the 90s do with AI? One of the most classic examples and applications of early machine learning is the recognition of handwritten digits. Reading these digits is simple and redundant work, so people want to do it automatically, but how? The three building blocks of AI are algorithms, data, and computing infrastructure (citation needed!). Now, assuming we don’t need to worry about computing resources, what else do we need? Data: Fortunately, we have the MNIST database, which has 60000 processed training images and 10000 processed test images! Algorithms: Mathematicians have been working on these things for years, so we have algorithms! In this example, we’re going to use the K-Nearest Neighbour algorithm to classify hand written digits.The project was actually a final coursework for us in the TB-1, and as someone interested in AI, I can’t tell how excited I was! What is K-Nearest Neighbour?Let’s get started first by looking into the K-Nearest Neighbour algorithm. Here’s an example to help you understand it better. Suppose the Stultavalo Gardens High School (near Jing’an Temple) only has two kinds of flowers, camelia and sakura blossom. A robot was sent into the gardens to classify each of the flowers it saw using two features: the size and the colour. The robot already had some flowers classified in its database and came across a new flower.After analyzing the six other flowers in the database that are most similar to the new flower, the robot determined that the unknown flower was more alike in colour to four known camellias and two other sakura and in size to five camellias and one sakura. Based on this information, what would be a reasonable guess? Well, it’s sensible to say that the unknown flower is a camellia, given that most of its neighbours are camellias. By using a 2-dimensional graph, we can see that the majority of the unknown flower’s six neighbours are camellias. Therefore, it’s reasonable to conclude that it is indeed a camellia. The two axes represent colour and size, respectively. Red points represent camellia samples, green points represent sakura blossom samples, and the black point is the unknown sample to be classified. This is a simple example of how the K-NN algorithm works: it identifies where the majority of an unknown’s neighbours belong. How does it work (mathematically)?Defining distanceHow are we going to find the k nearest neighbours of a point?Well, that’s easy. We could first calculate the distance between this point and all other points, then take the k points with minimal distance. Now comes the first task. How do we define distance? First, we have Euclidian distance, which describes the distance between two points in Euclidian space. It’s just the distance we often see in real life.For n-D vectors x and y:We also have other measurements of distances.Manhattan distance (a.k.a. taxicab distance) is the sum of the differences in all axes.Its 2-D version describes the distance for taking a taxicab from one place to another in a Manhattan-like block city. (Green line shows Euclidian distance) Chebyshev distance (a.k.a. chessboard distance) is the maximum value of all the differences in all axes.Its 2-D version describes the steps a king needs to take on a chessboard to get from one square to another. All three abovementioned measurements are special cases of Minkowski distance, with the parameter p of 2, 1 and , respectively. Cosine similarity is another measurement measuring the cosine of the angle between two vectors, which is often used in text mining and data mining.Hamming distance measures distances between strings. It is the number of steps to change one string to another.In this example, we use Euclidian distance. We use it not because it is easy but because it best fits our data. Reasons for EuclidianWhy matrices?Let’s code!Is there a better K for this?ConclusionReferencesIBM topic on KNNWikipedia pages","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2024-01-25T11:27:33.838Z","updated":"2024-01-28T15:16:20.588Z","comments":true,"path":"2024/01/25/hello-world/","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/2024/01/25/hello-world/","excerpt":"","text":"Welcome to Kozublog! I’m delighted to have you here! This blog is a space where I store what I learn, what I see and what I feel. I hope you’ll find inspiration as you explore the posts that I’ll be sharing. 2024-01-26 About the authorI am a first-year data science student at the university of Bristol. I’m pleased to share my insights!","categories":[],"tags":[]}],"categories":[],"tags":[]}