{"meta":{"title":"Kozublog","subtitle":"A site of data science-related blogs","description":"","author":"jiangyiqing","url":"https://jiangyiqing.github.io/kozublogs.github.io","root":"/kozublogs.github.io/"},"pages":[{"title":"About Me","date":"2024-01-29T13:03:13.770Z","updated":"2024-01-28T13:56:54.395Z","comments":true,"path":"about.html","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/about.html","excerpt":"","text":""}],"posts":[{"title":"C Project - Using K-Nearest Neighbour to Classify Hand-Written Digits","slug":"knn-mnist","date":"2024-01-28T14:04:02.895Z","updated":"2024-01-31T14:25:46.641Z","comments":true,"path":"2024/01/28/knn-mnist/","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/2024/01/28/knn-mnist/","excerpt":"","text":"C Project TB1Using K-Nearest Neighbour algorithm to classify hand-written digits. What are we going to do?Although AI can nowadays do fancy work, such as creating images and generating articles, it started from humble beginnings. I mean, it all stems from certain types of neural networks and models. Back in the 90s, when algorithms, data, and, most importantly, hardware were less developed than today, people seldom used neural networks in applications because they cost too much. Instead, they use statistical models. In my opinion, statistical algorithms could also be viewed as a type of artificial intelligence. After all, they handle unseen data. So, what did people in the 90s do with AI? One of the most classic examples and applications of early machine learning is the recognition of handwritten digits. Reading these digits is simple and redundant work, so people want to do it automatically, but how? The three building blocks of AI are algorithms, data, and computing infrastructure (citation needed!). Now, assuming we don’t need to worry about computing resources, what else do we need? Data: Fortunately, we have the MNIST database, which has 60000 processed training images and 10000 processed test images! Algorithms: Mathematicians have been working on these things for years, so we have algorithms! In this example, we’re going to use the K-Nearest Neighbour algorithm to classify hand written digits.The project was actually a final coursework for us in the TB-1, and as someone interested in AI, I can’t tell how excited I was! What is K-Nearest Neighbour?Let’s get started first by looking into the K-Nearest Neighbour algorithm. Here’s an example to help you understand it better. Suppose the Stultavalo Gardens High School (near Jing’an Temple) only has two kinds of flowers, camelia and sakura blossom. A robot was sent into the gardens to classify each of the flowers it saw using two features: the size and the colour. The robot already had some flowers classified in its database and came across a new flower.After analyzing the six other flowers in the database that are most similar to the new flower, the robot determined that the unknown flower was more alike in colour to four known camellias and two other sakura and in size to five camellias and one sakura. Based on this information, what would be a reasonable guess? Well, it’s sensible to say that the unknown flower is a camellia, given that most of its neighbours are camellias. By using a 2-dimensional graph, we can see that the majority of the unknown flower’s six neighbours are camellias. Therefore, it’s reasonable to conclude that it is indeed a camellia. The two axes represent colour and size, respectively. Red points represent camellia samples, green points represent sakura blossom samples, and the black point is the unknown sample to be classified. This is a simple example of how the K-NN algorithm works: it identifies where the majority of an unknown’s neighbours belong. How does it work (mathematically)?Defining distanceHow are we going to find the k nearest neighbours of a point?Well, that’s easy. We could first calculate the distance between this point and all other points, then take the k points with minimal distance. Now comes the first task. How do we define distance? First, we have Euclidian distance, which describes the distance between two points in Euclidian space. It’s just the distance we often see in real life.For n-D vectors x and y:We also have other measurements of distances.Manhattan distance (a.k.a. taxicab distance) is the sum of the differences in all axes.Its 2-D version describes the distance for taking a taxicab from one place to another in a Manhattan-like block city. (Green line shows Euclidian distance) Chebyshev distance (a.k.a. chessboard distance) is the maximum value of all the differences in all axes.Its 2-D version describes the steps a king needs to take on a chessboard to get from one square to another. All three abovementioned measurements are special cases of Minkowski distance, with the parameter p of 2, 1 and , respectively. Cosine similarity is another measurement measuring the cosine of the angle between two vectors, which is often used in text mining and data mining.Hamming distance measures distances between strings. It is the number of steps to change one string to another.For instance, “college” and “cottage” has the hamming distance of 3 because three chars from “college” need to be replaced to get “cottage”. In this example, we use Euclidian distance. We use it not because it is easy but because it best fits our data. Reasons for EuclidianLet’s first take a look at our dataset. The dataset consists of grayscale images, each with dimensions of 28x28 pixels, which means each pixel stores one value for how light this looks on the image. How do we measure the distance between two images?We could measure the pixel-wise distance between the images and sum the total distance. And that’s basically the logic of Euclidian distance.Meanwhile, since each value is an integer from 0 to 255, we could treat it as continuous data, which suits Euclidian.Finally, Euclidian distance is also sensitive to pixel value differences, which is helpful in detecting subtle features of different images. ( I still need to learn how to explain this mathematically. )Here, we show the Euclidian distance between the images: It seems (not so obviously) that the differences (bright pixels) between the “2” and “3” are more significant than those of the “3” and another “3” But in the third example, two “1”s look almost identical ( trust me, they are not the same image ), and their differences are negligibly insignificant. Why matrices?Since we only compare pixels in the same position of images, we could treat the images as vectors and compare each entry and sum their square to get the distance. It is often not the case for some more complicated algorithms.Now we have, the distance between two images:The next step is to do it on more samples. We have approximately 2000 training data and 100 test data in the coursework. Our goal is to calculate the distance between the 100 test images and the 2000 training images. So, 200,000 more calculations to go. That should be a piece of cake for modern computers.But where should we put these data? How should we store it? There’s only one way out. A table.The row of this table shows the distance between the test image and all the train images. dist X_train[0] X_train[1] X_train[2] X_train[3] … X_test[0] 143205 639402 850382 90932 … X_test[1] 291702 326314 20515 105710 … … … … … … … (I made this data up just for illustration.) Or should I say a matrix? We’ll call this a Distance Matrix , with entrieswhere is the row of the training set and is the row of the test set, is the Euclidian distance defined above. We could also treat the training and test sets as matrices, as we only need to use their rows. This will be useful when we start coding later. What’s next?So, now we have a matrix storing all the distances we need. What are the next steps?We shall find the smallest k distances corresponding to k nearest neighbours of the test image. By finding the tag corresponding to these nearest images, we can make a guess of which number the test sample might be.In this case, we’ll use k=5, as is required in the coursework. In those 5 nearest neighbours, if we have 3 or more than 3 neighbours having the same number, we can assume that the test sample should be labelled as that number. Let’s code!Now comes the coding part.In the coursework, we already have training sets, training labels and test sets stored in matrices, which is a struct defined as: 1234567struct matrix{ int numrow; int numcol; int *elements;};typedef struct matrix Matrix; This structure stores the number of rows and columns as integers and entries in an integer array. We can also define vector structure since we’ll be faced with vector operations. 123456struct vector{ int len; int *elements;};typedef struct vector Vector; Next we’ll write some helper functions.get_elem: a function for reading elements from a matrix.set_elem: a function for writing elements from a matrix.get_row: a function that reads a row from a matrix and store it as a vector.find_min_index and find_max_index: functions for finding the index of minimum and maximum in a vector. Generating the Distance MatrixThe Euclidian distance between two vectors is calculated via a for loop. 123456789int dist2_vec(Vector a, Vector b){ int sum = 0; for (int i = 0; i &lt; a.len; i++) { sum += (a.elements[i] - b.elements[i]) * (a.elements[i] - b.elements[i]); } return sum;} The function that reads rows from two matrices and constructs a new matrix, which is the distance matrix. 12345678910111213141516171819void pairwise_dist2(Matrix M1, Matrix M2, Matrix D){ // loop over each row of first matrix for (int i = 0; i &lt; M1.numrow; i++) { // loop over each row of the second matrix for (int j = 0; j &lt; M2.numrow; j++) { // choose two different rows form the matrices Vector a = get_row(M1, i); Vector b = get_row(M2, j); // calculate distances and assign value set_elem(D, i, j, dist2_vec(a, b)); // free heap memory free(a.elements); free(b.elements); } }} With these functions, we’ll be able to get the distance matrix, which stores all the distance information we need. Finding Neighbours and Making PredictionsThe 5 neighbours of the test image could be found by locating the label attached to the 5 smallest distances in a row of the distance matrix. Now, we need to find the 5 minimal indices of each row in the distance matrix. We could do this using our helper functions, especially find_min_index.First, we find the index of the smallest element. Then, we repeat the same process while replacing the smallest element with INT_MAX, the largest int in C. Then this element will no longer affact our min-finding process, so we’ll have the index of the second smallest element. 123456789void minimum5(int a[], int len, int indices[]){ for (int i = 0; i &lt; 5; i++) { int min_idx = find_min_index(a, len); indices[i] = min_idx; a[min_idx] = INT_MAX; }} Finally, we map the X_Train training sets to Y_Train label sets and make the prediction by judging whether there are more than 2 neighbours having the same label. However, that means, sometimes, we’ll find samples that can’t be categorised into the 0~9 digits. Is there a better K for this?The short answer is: It depends. In machine learning, model makers face the bias-variance trade-off. The higher the bias, the simpler the model, and the lower the variance. Simple models can’t handle so many features (underfitting), but models too complex are distracted by noises (overfitting).In the k-nearest neighbours model, the bias-variance trade-off is highly affected by the choice of k. For smaller k ( such as k=1 ), the model tends to have a small bias, resulting in more complex models. But it also brings a high variance, meaning the model is sensitive to small changes. Namely, having a smaller k tends to cause overfitting. Here’s an example of an over-complicated model from the book The Elements of Statistical Learning.For larger k, the model is large in bias and small in variance, meaning it might be oversimplifying the problem. That is to say, a larger k tends to cause cause underfitting. Here’s an example of a larger k from the book The Elements of Statistical Learning. This model seems to be pretty fine. So, which k to choose depends on what kind of model we want. In order that we find the best k, we first need to define a standard for model performance check. Then, we can apply cross-validation and find the best k we need. What else can we do?Recall the example where two “3”s have a sizeable Euclidian difference. From the perspective of humans, we know that clearly, these two digits are almost the same thing, except for the fact that one is rotating to the left and the other is rotating to the right.Scientists found the same issue back in the 90s, so they implemented a method that could resolve this: tangent distance.However, this is beyond my knowledge ( for the time being ), so I will not write about this in this article. ReferencesIBM topic on KNNWikipedia pagesThe Elements of Statistical Learning","categories":[{"name":"Coursework Review","slug":"Coursework-Review","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/categories/Coursework-Review/"}],"tags":[{"name":"C Programming","slug":"C-Programming","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/C-Programming/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/Machine-Learning/"},{"name":"KNN","slug":"KNN","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/KNN/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-01-25T11:27:33.838Z","updated":"2024-01-28T15:16:20.588Z","comments":true,"path":"2024/01/25/hello-world/","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/2024/01/25/hello-world/","excerpt":"","text":"Welcome to Kozublog! I’m delighted to have you here! This blog is a space where I store what I learn, what I see and what I feel. I hope you’ll find inspiration as you explore the posts that I’ll be sharing. 2024-01-26 About the authorI am a first-year data science student at the university of Bristol. I’m pleased to share my insights!","categories":[],"tags":[]}],"categories":[{"name":"Coursework Review","slug":"Coursework-Review","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/categories/Coursework-Review/"}],"tags":[{"name":"C Programming","slug":"C-Programming","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/C-Programming/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/Machine-Learning/"},{"name":"KNN","slug":"KNN","permalink":"https://jiangyiqing.github.io/kozublogs.github.io/tags/KNN/"}]}